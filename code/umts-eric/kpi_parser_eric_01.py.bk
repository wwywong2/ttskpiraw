#!/usr/bin/python

## Imports
import sys
import os
import time
import uuid

from pyspark import SparkConf, SparkContext
from pyspark import SparkFiles

from random import random
from operator import add

import socket

from xmlparser_eric import XMLParser



## Constants
socket_retry_sec = 5 # num of sec to wait for next retry when socket fail
socket_retry_num = 10 # num of times to retry when socket fail



# set some key var
curr_py_path = os.path.realpath(__file__) # current running file - abs path
curr_py_dir, curr_py_filename = os.path.split(curr_py_path)  # current file and folder - abs path
#curr_py_dir = os.path.dirname(curr_py_path)

# argv[1] - input file
# argv[2] - process name
# argv[3] - output dir


APP_NAME = "Read seq xml file (w/ map reduce)"
# take app name from param
if len(sys.argv) >= 3:
   APP_NAME = sys.argv[2]

# output dir
output_dir = ""
if len(sys.argv) >= 4:
   output_dir = sys.argv[3]
output_dir = output_dir.rstrip('/')
if output_dir == "":
   output_dir = "." # default current folder
elif not os.path.isdir(output_dir): # create if not exist
   try:	
      os.mkdir(output_dir)
   except:
      print '[%s] Failed to create folder \"%s\"!' % (
      	time.strftime("%Y-%m-%d %H:%M:%S"), output_dir)
      print "[%s] Process terminated." % (
	 	time.strftime("%Y-%m-%d %H:%M:%S"))
      sys.stdout.flush()
      sys.exit(2)
else:
   pass

   








##OTHER FUNCTIONS/CLASSES

def f_map(filetuple):
   [fn,bw] = filetuple

   try:
      xml = XMLParser(fn,'bytes','file',bw,SparkFiles.get('config.ini'))
      return xml.ProcessXML()
   except Exception as e:
      print "[%s] err in file: %s" % (
         time.strftime("%Y-%m-%d %H:%M:%S"), fn)
      sys.stdout.flush()
      return ""

def f_reduce(a, b):
   return a + b



def main(sc,filename):

   '''
   # sample
   conf = SparkConf().setAppName(APP_NAME) \
      .setMaster("spark://master:7077") \
      .set("spark.executor.memory", "1g") \
      .set("spark.driver.memory", "1g") \
      .set("spark.executor.cores", "1") \
      .set("spark.cores.max", num_core)
   '''


   # read ini
   configIni = XMLParser.ReadConfigIni(curr_py_dir+'/config.ini')
   global sc_configIni
   sc_configIni = sc.broadcast(configIni)

   # test add file
   sc.addFile(curr_py_dir+'/config.ini')

   # test add py reference
   sc.addPyFile(curr_py_dir+'/xmlparser_eric.py')

   # read file
   print "[%s] reading file: %s" % (
      time.strftime("%Y-%m-%d %H:%M:%S"), filename)
   sys.stdout.flush()
   textRDD = sc.sequenceFile(filename)
   print "[%s] finish reading file: %s" % (
      time.strftime("%Y-%m-%d %H:%M:%S"), filename)
   sys.stdout.flush()
   #print textRDD.collect()

   curr_try_num = 0
   bSocketConnFail = True # init to True so it will go into loop
   while (curr_try_num < socket_retry_num and bSocketConnFail):

      try:

         # map
         mapRDD = textRDD.map(f_map)
         #print mapRDD.count()
         #print mapRDD.collect()

         # reduce
         redRDD = mapRDD.reduce(f_reduce)
         #print redRDD.count()
         #print redRDD.collect()

      except socket.error as e:

         print "[%s] Job: %s: Socket Error: %s!" % (
            time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, e)
         sys.stdout.flush()
         curr_try_num += 1 # increment retry count
         if curr_try_num < socket_retry_num:
            print "[%s] Job: %s: will retry in %d sec" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_sec)
            sys.stdout.flush()
            time.sleep(socket_retry_sec)
         else:
            print "[%s] Job: %s: too many retry (%d)! Give up!" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_num)
            sys.stdout.flush()

      except socket.timeout as e:

         print "[%s] Job: %s: Socket Timeout: %s!" % (
            time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, e)
         sys.stdout.flush()
         curr_try_num += 1 # increment retry count
         if curr_try_num < socket_retry_num:
            print "[%s] Job: %s: will retry in %d sec" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_sec)
            sys.stdout.flush()
            time.sleep(socket_retry_sec)
         else:
            print "[%s] Job: %s: too many retry (%d)! Give up!" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_num)
            sys.stdout.flush()

      except Exception as e: # trying to catch could not open socket

         if hasattr(e, 'message') and e.message == 'could not open socket':

            print "[%s] Job: %s: Socket Error: %s!" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, e)
            sys.stdout.flush()
            curr_try_num += 1 # increment retry count
            if curr_try_num < socket_retry_num:
               print "[%s] Job: %s: will retry in %d sec" % (
                  time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_sec)
               sys.stdout.flush()
               time.sleep(socket_retry_sec)
            else:
               print "[%s] Job: %s: too many retry (%d)! Give up!" % (
                  time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, socket_retry_num)
               sys.stdout.flush()

         else:

            print "[%s] Job: %s: Other Exception Error: %s!" % (
               time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, e)
            sys.stdout.flush()

            raise # not the error we are looking for

      except:

         print "[%s] Job: %s: Other Unknown Error: %s!" % (
            time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME, e)
         sys.stdout.flush()

         raise # not the error we are looking for

      else:

         bSocketConnFail = False




   if bSocketConnFail: # max retry reached, still fail
      print "[%s] socket issue(s), failed parsing job: %s" % (
         time.strftime("%Y-%m-%d %H:%M:%S"), APP_NAME)
      sys.stdout.flush()
      return




   # print to file
   input_dir, input_filename = os.path.split('./' + filename + '.txt')  # current file and folder - abs path
   output_filename = output_dir + '/' + input_filename
   err_filename = output_filename.split('.txt')[0] + '.error.txt'

   print "[%s] start writing file: %s" % (
      time.strftime("%Y-%m-%d %H:%M:%S"), output_filename)
   sys.stdout.flush()

   err = ""
   rslt = ""
   with open(output_filename,'w') as fout:
      uuidstr = str(uuid.uuid4())
      mycache = list()
      header = XMLParser.GetHeader(curr_py_dir+'/config.ini')
      fout.write(header+'\n')
      [rslt, err] = XMLParser.GetReport(redRDD, mycache)
      for row in rslt.split("\n"):
         if len(row) > 1:
            fout.write(uuidstr+','+row+'\n')

   # output error log
   with open(err_filename,'w') as ferr:
      ferr.write(err)



   print "[%s] finish writing file: %s" % (
      time.strftime("%Y-%m-%d %H:%M:%S"), output_filename)
   sys.stdout.flush()





if __name__ == "__main__":

   #print "before sc"

   # Configure Spark
   conf = SparkConf().setAppName(APP_NAME)
   conf = conf.setMaster("spark://master:7077")
   sc   = SparkContext(conf=conf)
   filename = sys.argv[1]

   # Execute Main functionality
   main(sc, filename)
   sc.stop()
